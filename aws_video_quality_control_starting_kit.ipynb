{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Grid Dynamics International, Inc. All Rights Reserved\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import math\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're loading python libraries which will help us conduct this demo prototype using AWS. Numpy is a standard library focused on matrix manipulations in Python. CV2 or OpenCV is a standard computer vision library which is used for a wide variety of image processing tasks in Python. OS allows us to automate access to file directories and file structures for retrieving files in Python. Matplotlib is a standard charting library in Python.\n",
    "\n",
    "Math gives us access to some advanced math functionality. TQDM is a lightweight process bar for python. Time allows us to measure time taken for function calls in Python (Time based Profiling). \n",
    "\n",
    "Boto3 is a AWS SDK Library which allows us to call different functionalities from AWS right inside this jupyter notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video I/O functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some of the standard functions in Python to make the video usable for analysis along with AWS Lookout for Vision. split_video takes as input a path to the video, a sample rate to sample the video at and an output directory to push image frames extracted from the video at the sampling rate. \n",
    "\n",
    "split_video supports codecs supported by opencv only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video(input_video_path, output_dir, sample_rate=240):\n",
    "    '''\n",
    "    input_video_path: Path to Input Video\n",
    "    output_dir: Path to Directory for Storing Image Frames\n",
    "    sample_rate: Sampling rate to extract frames from video (240 on a 60 fps video = 1 frame every 4 s)\n",
    "    '''\n",
    "    video_capture = cv2.VideoCapture(input_video_path)\n",
    "    result = []\n",
    "    \n",
    "    frame_id = 0\n",
    "    while True:\n",
    "        success, image = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if (frame_id % sample_rate) == 0:\n",
    "            image_name = f\"frame_{frame_id}\"\n",
    "            save_path = os.path.join(output_dir, f\"{image_name}.jpg\")\n",
    "            cv2.imwrite(save_path, image)\n",
    "            result.append((image_name, save_path))\n",
    "        frame_id += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the entire process of extracting the images from the video, identifying objects of interest, labeling anomalies, ,tracking objects and detecting anomalies is completed for a complete demo we provide functions to convert individual frames to a video using some of the standard codecs available within opencv. \n",
    "\n",
    "Save_video_from_frames takes in a list(called frames) of tuples of image name, image path referring to the frame name and frame paths and a save_path for saving the video.\n",
    "\n",
    "save_video_from_frames supports codecs supported by opencv only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_from_frames(frames, save_path, fps=60):\n",
    "    '''\n",
    "    frames: list of image name, image path\n",
    "    save_path: Path/Directory to save the rendered video to.\n",
    "    '''\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = None\n",
    "\n",
    "    for frame_name, frame_path in tqdm(frames):\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if out is None:\n",
    "            out = cv2.VideoWriter(save_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "        out.write(frame.astype('uint8'))\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for annotation and prediction using AWS Lookout for Vision. Since we're working off a video, the objects in question may not be suitable organised for an expert to review. In addition, objects may be out of focus or there may be multiple objects per frame of the video. Therefore, here we demonstrate a non-learning based approach to detect objects of interest in individual frames of an image and how to refine this approach further. We then cut-out the objects of interest from the relevant frames and store them, saving them for further processing and then to train an anomaly detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHADOWS_LBC = (65, 55, 30)\n",
    "SHADOWS_UBC = (95, 85, 40)\n",
    "SHADOWS_MORPH_KER_SZ = 5\n",
    "SHADOWS_DIL_ITER = 4\n",
    "EDGES_THRESH_1 = 50\n",
    "EDGES_THRESH_2 = 85\n",
    "EDGES_MORPH_KER_SZ = 7\n",
    "EDGES_DIL_ITER = 1\n",
    "BACKGROUND_THRESH = 80\n",
    "OBJECTS_MORPH_ERODE_KER_SZ = 5\n",
    "OBJECTS_MORPH_DILATE_KER_SZ = 3\n",
    "AREA_LOWER_BOUND = 1800\n",
    "BOX_INCR_PERCENT = 0.1\n",
    "AREA_INTERSECTION_THRESH = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a set of parameters defined in order for the bounding box based object detection method to work smoothly.\n",
    "\n",
    "SHADOWS_LBC, SHADOWS_UBC - Upper and lower bounds for selecting the shadows mask\n",
    "\n",
    "SHADOWS_MORPH_KER_SZ - Kernel Size for the morphological dilation of the shadows mask\n",
    "(dilation - slight increasing of a mask + joining \"broken\" disjoint parts of shadows)\n",
    "\n",
    "EDGES_THRESH_1 - first threshold for the hysteresis procedure for the Canny Edge Detector\n",
    "\n",
    "EDGES_THRESH_2 - second threshold for the hysteresis procedure for the Canny Edge Detector\n",
    "\n",
    "EDGES_MORPH_KER_SZ - Kernel Size for the morphological dilation of the edges mask\n",
    "\n",
    "OBJECTS_MORPH_ERODE_KER_SZ, OBJECTS_MORPH_DILATE_KER_SZ - Kernel Size for the morphological opening transformation (erodion followed by dilation, used to remove noise and detach individual objects, that are too close)\n",
    "\n",
    "AREA_LOWER_BOUND - minimum area to consider for detection of an object\n",
    "\n",
    "BOX_INCR_PERCENT - Additonal increment percentage for the box size\n",
    "\n",
    "AREA_INTERSECTION_THRESH - a threshold to consider two intersecting boxes to belong to the same object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects_bboxes(image, area_lower_bound):    \n",
    "    '''\n",
    "    image: Pass an image onto this function\n",
    "    area_lower_bound: exclude detected objects which are smaller than area lower bound (measured in pixels)\n",
    "    '''\n",
    "    edges = cv2.Canny(image, EDGES_THRESH_1, EDGES_THRESH_2)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (EDGES_MORPH_KER_SZ, EDGES_MORPH_KER_SZ))\n",
    "    edges = cv2.dilate(edges, kernel, iterations=EDGES_DIL_ITER)\n",
    "    \n",
    "    shadows = cv2.inRange(image, np.asarray(SHADOWS_LBC), np.asarray(SHADOWS_UBC))\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (SHADOWS_MORPH_KER_SZ, SHADOWS_MORPH_KER_SZ))\n",
    "    shadows = cv2.dilate(shadows, kernel, iterations=SHADOWS_DIL_ITER)\n",
    "    \n",
    "    imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, background = cv2.threshold(imgray, BACKGROUND_THRESH, 255, 0)\n",
    "    \n",
    "    objects = cv2.bitwise_not(cv2.bitwise_or(background, cv2.bitwise_or(shadows, edges)))\n",
    "    \n",
    "    kernel_e = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,\n",
    "                                         (OBJECTS_MORPH_ERODE_KER_SZ, OBJECTS_MORPH_ERODE_KER_SZ))\n",
    "    kernel_d = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,\n",
    "                                         (OBJECTS_MORPH_DILATE_KER_SZ, OBJECTS_MORPH_DILATE_KER_SZ))\n",
    "\n",
    "    objects = cv2.morphologyEx(objects, cv2.MORPH_ERODE, kernel_e)\n",
    "    objects = cv2.morphologyEx(objects, cv2.MORPH_DILATE, kernel_d)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(objects, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    top_level_mask = (hierarchy[0, :, 3] == -1)\n",
    "    \n",
    "    area = np.array([cv2.contourArea(ctr) for ctr in contours])\n",
    "    area_mask = (area > area_lower_bound)\n",
    "\n",
    "    mask = top_level_mask & area_mask\n",
    "    objects_contours = np.array(contours, dtype=object)[mask]\n",
    "    \n",
    "    boxes = [cv2.boundingRect(contour) for contour in objects_contours]\n",
    "    \n",
    "    increased_boxes = []\n",
    "    for x, y, w, h in boxes:\n",
    "        w_add = w * BOX_INCR_PERCENT\n",
    "        h_add = h * BOX_INCR_PERCENT\n",
    "        x_inc = max(0, x - w_add)\n",
    "        y_inc = max(0, y - h_add)\n",
    "        w_inc = w + 2 * w_add\n",
    "        h_inc = h + 2 * h_add\n",
    "        increased_boxes.append((x_inc, y_inc, w_inc, h_inc))\n",
    "    \n",
    "    return increased_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookout for vision related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 384\n",
    "IMAGE_HEIGHT = 384\n",
    "PAD_COLOR = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we get bounding boxes and co-ordinates from our prior pipeline, what we need to now do is to crop those objects of interest from the image frames. Once this is done, since AWS Lookout for Vision expects all the images to be of the same size, we resize the images to be of the same size before the images are sent to be labeled using AWS Lookout for Vision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the code from AWS blogpost here:\n",
    "# https://aws.amazon.com/blogs/machine-learning/computer-vision-based-anomaly-detection-using-amazon-lookout-for-vision-and-aws-panorama/\n",
    "def crop_n_resize_image(img, bbox, size, padColor=0):\n",
    "    '''\n",
    "    img: Image to be modified\n",
    "    bbox: bbox dimensions \n",
    "    size: Output size of the resized image\n",
    "    padColor: If there is a specification for Pad Color in (R,G,B)\n",
    "    '''\n",
    "    # crop images ==============================\n",
    "    crop = img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]].copy()\n",
    "    \n",
    "    # cropped image size\n",
    "    h, w = crop.shape[:2]\n",
    "    # designed crop image sizes\n",
    "    sh, sw = size\n",
    "\n",
    "    # interpolation method\n",
    "    if h > sh or w > sw: # shrinking image\n",
    "        interp = cv2.INTER_AREA\n",
    "    else: # stretching image\n",
    "        interp = cv2.INTER_CUBIC\n",
    "\n",
    "    # aspect ratio of image\n",
    "    aspect = w/h \n",
    "\n",
    "    # compute scaling and pad sizing\n",
    "    if aspect > 1: # horizontal image\n",
    "        new_w = sw\n",
    "        new_h = np.round(new_w/aspect).astype(int)\n",
    "        pad_vert = (sh-new_h)/2\n",
    "        pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
    "        pad_left, pad_right = 0, 0\n",
    "    elif aspect < 1: # vertical image\n",
    "        new_h = sh\n",
    "        new_w = np.round(new_h*aspect).astype(int)\n",
    "        pad_horz = (sw-new_w)/2\n",
    "        pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
    "        pad_top, pad_bot = 0, 0\n",
    "    else: # square image\n",
    "        new_h, new_w = sh, sw\n",
    "        pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "    # set pad color\n",
    "    if len(img.shape) == 3 and not isinstance(padColor, (list, tuple, np.ndarray)): # color image but only one color provided\n",
    "        padColor = [padColor] * 3\n",
    "\n",
    "    # scale and pad\n",
    "    scaled_img = cv2.resize(crop, (new_w, new_h), interpolation=interp)\n",
    "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=padColor)\n",
    "\n",
    "    return scaled_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function to call Lookout For Vision and then detect anomalies in the images. For this we need to first encode the images into byte format, invoke the AWS Lookout for Vision client and link to the trained model. The model then returns a response. The response consists of two details which we are interested in:\n",
    "1. IsAnomalous?\n",
    "2. Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(lookout_client, image, project_name, model_version):\n",
    "    '''\n",
    "    lookout_client: Reference to AWS Lookout for Vision Client\n",
    "    image: Image to be sent to AWS Lookout for Vision\n",
    "    project_name: Reference to project name\n",
    "    model_version: Refer to which Anomaly Detection model on AWS Lookout for Vision is to be used\n",
    "    '''\n",
    "    image_bytes = cv2.imencode('.jpg', image)[1].tobytes()\n",
    "    response=lookout_client.detect_anomalies(ProjectName=project_name, ContentType='image/jpeg', Body=image_bytes,\n",
    "                                             ModelVersion=model_version)\n",
    "    return response['DetectAnomalyResult']['IsAnomalous'], response['DetectAnomalyResult']['Confidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-object tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_RATE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECTION_RATE - rate to use object detection X frames (mainly to detect new objects that enters the frame)\n",
    "\n",
    "DST_THRESHOLD - distance threshold to consider two boxes belong to the same object (to not count twice objects that were already detected previosly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center(box):\n",
    "    '''\n",
    "    box: a bounding box in X, Y, W, H format\n",
    "    returns a center of a bounding box\n",
    "    '''\n",
    "    return box[0] + box[2] / 2, box[1] + box[3] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTracker:\n",
    "    '''\n",
    "    tracks bounding boxes for detected objects through the video\n",
    "    new objects should be initializes using add_boxes function (takes object detection results as an input)\n",
    "    '''\n",
    "    def __init__(self, inactive_thresh=2, upd_area_thresh=2.0, dst_thresh=100,\n",
    "                 tracker_func=cv2.legacy.TrackerKCF_create):\n",
    "        '''\n",
    "        inactive_thresh: inactive objects threshold - if object is not present on the frame in {inactive_thresh}\n",
    "          continious updates, drop it\n",
    "        upd_area_thresh: a lower bound of are fraction to replace a box if a larger one was detected for\n",
    "          the same object\n",
    "        dst_thresh: distance threshold to consider two boxes belong to the same object\n",
    "        tracker_func: OpenCV tracker creation function for individual objects\n",
    "        '''\n",
    "        self.trackers = []\n",
    "        self.boxes = []\n",
    "        self.inactive_time = []\n",
    "        self.inactive_thresh = inactive_thresh\n",
    "        self.upd_area_thresh = upd_area_thresh\n",
    "        self.dst_thresh = dst_thresh\n",
    "        self.tracker_func = tracker_func\n",
    "        \n",
    "        \n",
    "    def get_objects(self):\n",
    "        '''\n",
    "        returns a list of bounding boxes for all objects on the frame\n",
    "        '''\n",
    "        result = []\n",
    "        for obj_id, box in enumerate(self.boxes):\n",
    "            if self.inactive_time[obj_id] == 0:\n",
    "                result.append((obj_id, box))\n",
    "        return result\n",
    "\n",
    "    def _is_same_obj(self, box1, box2):\n",
    "        '''\n",
    "        checks if 2 boxes represent the same object (based on centroid distance)\n",
    "        box1, box2: bounding boxes in X, Y, W, H format\n",
    "        '''\n",
    "        cx1, cy1 = get_center(box1)\n",
    "        cx2, cy2 = get_center(box2)\n",
    "        dist = math.hypot(cx1 - cx2, cy1 - cy2)\n",
    "        return dist < self.dst_thresh\n",
    "    \n",
    "    \n",
    "    def _create_tracker(self, box, frame):\n",
    "        '''\n",
    "        initializes a new tracker for a newly found object\n",
    "        box: a bounding box in X, Y, W, H format\n",
    "        frame: the entire frame image \n",
    "        '''\n",
    "        tracker = self.tracker_func()\n",
    "        tracker.init(frame, box)\n",
    "        return tracker\n",
    "        \n",
    "        \n",
    "    def add_boxes(self, boxes, frame):\n",
    "        '''\n",
    "        adds all new detected boxes to the tracker (creates trackers for them);\n",
    "        ignores boxes for already tracked objects\n",
    "        boxes: a list of boxes in X, Y, W, H format\n",
    "        frame: the entire frame image \n",
    "        '''\n",
    "        for box in boxes:\n",
    "            found = False\n",
    "            for obj_id, existing_box in enumerate(self.boxes):\n",
    "                if self.inactive_time[obj_id] != 0:\n",
    "                    continue\n",
    "                if self._is_same_obj(box, existing_box):\n",
    "                    if box[2] * box[3] >= existing_box[2] * existing_box[3] * self.upd_area_thresh:\n",
    "                        # the new box is significantly bigger, replace the old one with the new one\n",
    "                        self.trackers[obj_id] = self._create_tracker(box, frame)\n",
    "                        self.inactive_time[obj_id] = 0\n",
    "                        self.boxes[obj_id] = box\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                # this box is a new one -> create a new tracker for this object\n",
    "                self.trackers.append(self._create_tracker(box, frame))\n",
    "                self.inactive_time.append(0)\n",
    "                self.boxes.append(box)\n",
    "                    \n",
    "\n",
    "    def update(self, frame):\n",
    "        '''\n",
    "        updates existing object positions based on a new frame\n",
    "        frame: a next frame image\n",
    "        '''\n",
    "        for obj_id, tracker in enumerate(self.trackers):\n",
    "            if self.inactive_time[obj_id] < self.inactive_thresh:\n",
    "                success, bbox = tracker.update(frame)\n",
    "                if success:\n",
    "                    self.inactive_time[obj_id] = 0\n",
    "                    self.boxes[obj_id] = bbox\n",
    "                else:\n",
    "                    self.inactive_time[obj_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to make a demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(img, info):\n",
    "    '''\n",
    "    adds text info to the bottom of the image\n",
    "    img: an image to add info\n",
    "    info: list of key, value pairs to print on the image\n",
    "    '''\n",
    "    H, W = img.shape[:2]\n",
    "    \n",
    "    cols = 2\n",
    "    rows = (len(info) + cols - 1) // cols\n",
    "    scale = 1.4\n",
    "    color = (0, 0, 0)\n",
    "    thickness = 2\n",
    "    \n",
    "    rh = 50 + rows * 70\n",
    "    rw = W\n",
    "    text_box = np.ones((rh, rw, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    for (i, (k, v)) in enumerate(info):\n",
    "        c = i // rows\n",
    "        r = i % rows\n",
    "        x = 50 + ((rw // cols - 100) * c)\n",
    "        y = 70 + r * 70\n",
    "        text = \"{}: {}\".format(k, v)\n",
    "        cv2.putText(text_box, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, color, thickness)\n",
    "        \n",
    "    return cv2.vconcat([img, text_box])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames_with_tracking_boxes(frames, tracking_ad_results, output_result_dir):\n",
    "    '''\n",
    "    adds bounding boxes and text info to the video frames, saves it to a new folder\n",
    "    frames: a list of frames with their names (each frame is represented as a pair: a name and a path to the image)\n",
    "    tracking_ad_results: dict with all tracking and anomaly detection results; keys are frame names and values are\n",
    "        lists of all bounding boxes, each bounding box is a tuple (idx, box, is_anomaly, confidence)\n",
    "            idx - object id, persisted between frames\n",
    "            box - bounding box in X, Y, W, H format\n",
    "            is_anomaly - boolean flag for anomaly classification\n",
    "            confidence - AWS Lookout for Vision confidence of resulting verdict\n",
    "    output_result_dir: a directory to save resulting video frames\n",
    "    '''\n",
    "    result_frames = []\n",
    "    objects_total = set()\n",
    "    anomalies_total = set()\n",
    "    for frame_name, frame_path in tqdm(frames):\n",
    "        if frame_name not in tracking_ad_results:\n",
    "            break\n",
    "        objects_frame = set()\n",
    "        anomalies_frame = set()\n",
    "        \n",
    "        frame = cv2.imread(frame_path)\n",
    "        boxes = tracking_ad_results[frame_name]\n",
    "\n",
    "        for idx, box, is_anomaly, confidence in boxes:\n",
    "            x, y, w, h = box\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            if confidence is None:\n",
    "                color = (255, 0, 0)\n",
    "                box_name = str(idx)\n",
    "            else:\n",
    "                anomaly_prob = confidence if is_anomaly else 1 - confidence\n",
    "                color = (0, 255 * (1 - anomaly_prob), 255 * anomaly_prob)\n",
    "                box_name = '{} {} {:.1f}'.format(idx, \"A\" if is_anomaly else \"N\", confidence)\n",
    "            cv2.putText(frame, box_name, (x, y - 15), cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "            objects_total.add(idx)\n",
    "            objects_frame.add(idx)\n",
    "            if is_anomaly:\n",
    "                anomalies_total.add(idx)\n",
    "                anomalies_frame.add(idx)\n",
    "        \n",
    "        info = [\n",
    "            (\"#objects total\", len(objects_total)),\n",
    "            (\"#anomalies total\", len(anomalies_total)),\n",
    "            (\"anomalies percentage total\", '{:.2f}'.format(len(anomalies_total) / len(objects_total) * 100)),\n",
    "            (\"#objects in the frame\", len(objects_frame)),\n",
    "            (\"#anomalies in the frame\", len(anomalies_frame)),\n",
    "            (\"anomalies percentage in the frame\", '{:.2f}'.format(len(anomalies_frame) / len(objects_frame) * 100)),\n",
    "        ]\n",
    "        frame = add_info(frame, info)\n",
    "\n",
    "        save_path = os.path.join(output_result_dir, f'{frame_name}.jpg')\n",
    "        cv2.imwrite(save_path, frame)\n",
    "\n",
    "        result_frames.append((frame_name, save_path))\n",
    "    return result_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train an AWS Lookout for Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"lookoutvision-us-east-1-017e8d732b\"\n",
    "project = \"anomaly_detection_demo\"\n",
    "dataset_folder = 'datasets/cookies_bboxes_01_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('lookoutvision', region_name='us-east-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is based on https://github.com/aws-samples/amazon-lookout-for-vision/blob/main/Amazon%20Lookout%20for%20Vision%20Lab.ipynb\n",
    "\n",
    "Note: Training a model can (will) take a few hours as it uses Deep Learning in the background. Once your model is trained, you can continue with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating project:anomaly_detection_demo\n",
      "project ARN: arn:aws:lookoutvision:us-east-1:313307525435:project/anomaly_detection_demo\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Creating project:' + project)\n",
    "response=client.create_project(ProjectName=project)\n",
    "print('project ARN: ' + response['ProjectMetadata']['ProjectArn'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Dataset Status: CREATE_IN_PROGRESS\n",
      "Dataset Status Message: The dataset is creating.\n",
      "Dataset Type: train\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Creating training dataset\n",
    "dataset_type ='train'\n",
    "manifest_file = os.path.join(dataset_folder, 'annotations_train.manifest')\n",
    "\n",
    "print('Creating dataset...')\n",
    "dataset=json.loads('{ \"GroundTruthManifest\": { \"S3Object\": { \"Bucket\": \"' + bucket + '\", \"Key\": \"'+ manifest_file + '\" } } }')\n",
    "\n",
    "response=client.create_dataset(ProjectName=project, DatasetType=dataset_type, DatasetSource=dataset)\n",
    "print('Dataset Status: ' + response['DatasetMetadata']['Status'])\n",
    "print('Dataset Status Message: ' + response['DatasetMetadata']['StatusMessage'])\n",
    "print('Dataset Type: ' + response['DatasetMetadata']['DatasetType'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Dataset Status: CREATE_IN_PROGRESS\n",
      "Dataset Status Message: The dataset is creating.\n",
      "Dataset Type: test\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_type ='test'\n",
    "manifest_file = os.path.join(dataset_folder, 'annotations_test.manifest')\n",
    "\n",
    "print('Creating dataset...')\n",
    "dataset=json.loads('{ \"GroundTruthManifest\": { \"S3Object\": { \"Bucket\": \"' + bucket + '\", \"Key\": \"'+ manifest_file + '\" } } }')\n",
    "\n",
    "response=client.create_dataset(ProjectName=project, DatasetType=dataset_type, DatasetSource=dataset)\n",
    "print('Dataset Status: ' + response['DatasetMetadata']['Status'])\n",
    "print('Dataset Status Message: ' + response['DatasetMetadata']['StatusMessage'])\n",
    "print('Dataset Type: ' + response['DatasetMetadata']['DatasetType'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating/training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "ARN: arn:aws:lookoutvision:us-east-1:313307525435:model/anomaly_detection_demo/1\n",
      "Version: 1\n",
      "Status: TRAINING\n",
      "Message: The model is being trained.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "output_bucket = bucket\n",
    "output_folder = os.path.join('projects', project, 'models/')\n",
    "\n",
    "    \n",
    "print('Creating model...')\n",
    "output_config=dataset=json.loads('{ \"S3Location\": { \"Bucket\": \"' + output_bucket + '\", \"Prefix\": \"'+ output_folder + '\" } } ')\n",
    "\n",
    "response=client.create_model(ProjectName=project, OutputConfig=output_config)\n",
    "print('ARN: ' + response['ModelMetadata']['ModelArn'])\n",
    "print('Version: ' + response['ModelMetadata']['ModelVersion'])\n",
    "print('Status: ' + response['ModelMetadata']['Status'])\n",
    "print('Message: ' + response['ModelMetadata']['StatusMessage'])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = response['ModelMetadata']['ModelVersion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Done!\n"
     ]
    }
   ],
   "source": [
    "while client.describe_model(ProjectName=project, ModelVersion=model_version)['ModelDescription']['Status']!='TRAINED':\n",
    "    print('.',end='')\n",
    "    time.sleep(5)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define I/O locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_video_path = 's3://lookoutvision-us-east-1-017e8d732b/input_video/cookies-01.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = os.path.join(workdir, 'cookies-01.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.join(workdir, 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames_dir = os.path.join(output_directory, 'video_frames')\n",
    "os.makedirs(output_frames_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result_dir = os.path.join(output_directory, 'tracking_frames')\n",
    "os.makedirs(output_result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_video_path = os.path.join(output_directory, 'pipeline_demo.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the input video from s3 to a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://lookoutvision-us-east-1-017e8d732b/input_video/cookies-01.mp4 to ../data/cookies-01.mp4\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {s3_input_video_path} {input_video_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split input video into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = split_video(input_video_path, output_frames_dir, sample_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an AWS Lookout client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelVersion': '1',\n",
       " 'ModelArn': 'arn:aws:lookoutvision:us-east-1:313307525435:model/anomaly_detection_demo/1',\n",
       " 'CreationTimestamp': datetime.datetime(2022, 9, 30, 16, 12, 51, 376000, tzinfo=tzlocal()),\n",
       " 'Status': 'TRAINED',\n",
       " 'StatusMessage': 'Training completed successfully.',\n",
       " 'Performance': {'F1Score': 0.856249988079071,\n",
       "  'Recall': 0.7611111402511597,\n",
       "  'Precision': 0.9785714149475098},\n",
       " 'OutputConfig': {'S3Location': {'Bucket': 'lookoutvision-us-east-1-017e8d732b',\n",
       "   'Prefix': 'projects/anomaly_detection_demo/models/'}},\n",
       " 'EvaluationManifest': {'Bucket': 'lookoutvision-us-east-1-017e8d732b',\n",
       "  'Key': 'projects/anomaly_detection_demo/models/EvaluationManifest-anomaly_detection_demo-1.json'},\n",
       " 'EvaluationResult': {'Bucket': 'lookoutvision-us-east-1-017e8d732b',\n",
       "  'Key': 'projects/anomaly_detection_demo/models/EvaluationResult-anomaly_detection_demo-1.json'},\n",
       " 'EvaluationEndTimestamp': datetime.datetime(2022, 9, 30, 17, 46, 11, 103000, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.describe_model(ProjectName=project,ModelVersion=model_version)['ModelDescription']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Lookout for vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: STARTING_HOSTING\n"
     ]
    }
   ],
   "source": [
    "response=client.start_model(ProjectName=project, ModelVersion=model_version, MinInferenceUnits=1)\n",
    "print('Status: ' + response['Status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................Done!\n"
     ]
    }
   ],
   "source": [
    "while client.describe_model(ProjectName=project,\n",
    "                            ModelVersion=model_version)['ModelDescription']['Status'] != 'HOSTED':\n",
    "    print('.',end='')\n",
    "    time.sleep(5)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main step: do object tracking and anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_coordinates(rect):\n",
    "    '''\n",
    "    returns bottom left and top right corners coordinates of a rectangle\n",
    "    rect: rectangle in X, Y, W, H format\n",
    "    '''\n",
    "    x1, y1, w, h = rect\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    x1, x2 = min(x1, x2), max(x1, x2)\n",
    "    y1, y2 = min(y1, y2), max(y1, y2)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def intersect_rectangles(rect1, rect2):\n",
    "    '''\n",
    "    finds intersection of 2 rectangles as a rectangle or None, if they don't intersect\n",
    "    rect1, rect2: rectangles in X, Y, W, H format\n",
    "    '''\n",
    "    l1, d1, r1, u1 = _get_coordinates(rect1)\n",
    "    l2, d2, r2, u2 = _get_coordinates(rect2)\n",
    "    l = max(l1, l2)\n",
    "    r = min(r1, r2)\n",
    "    d = max(d1, d2)\n",
    "    u = min(u1, u2)\n",
    "    if l < r and d < u:\n",
    "        return l, d, r - l, u - d\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_area(rect):\n",
    "    '''\n",
    "    finds area of a rectangle\n",
    "    rect: a rectangle in X, Y, W, H format\n",
    "    '''\n",
    "    x, y, w, h = rect\n",
    "    return w * h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects that touch left or right edge considered as partially visible (because in our scenario the \"conveyor belt\" moves from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_WIDTH_PERCENT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDGE_WIDTH_PERCENT - a width of left and right edge areas (in percentage of the total image width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_touches_edge(box, frame_shape):\n",
    "    '''\n",
    "    checks if a bounding box doesn't touch an image edge (left or right)\n",
    "    box: a bounding box in X, Y, W, H format\n",
    "    frame_shape: image size in H, W format\n",
    "    '''\n",
    "    x_min = frame_shape[1] * EDGE_WIDTH_PERCENT\n",
    "    x_max = frame_shape[1] - x_min\n",
    "    y_min = frame_shape[0] * EDGE_WIDTH_PERCENT\n",
    "    y_max = frame_shape[0] - y_min\n",
    "    x0, y0, w, h = box\n",
    "    x1 = x0 + w\n",
    "    y1 = y0 + h\n",
    "    return x_min <= x0 < x_max and x_min <= x1 < x_max\n",
    "#     return x_min <= x0 < x_max and y_min <= y0 < y_max and x_min <= x1 < x_max and y_min <= y1 < y_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main pipeline steps:\n",
    "* iterate over frames, detect new objects and track existing ones\n",
    "    * iterate over objects and drop overlapping boxes (rarely happens because of false positives in tracking algorithm)\n",
    "    * iterate over objects and assign anomaly classification labels (using AWS Lookout for vision for a new objects;\n",
    "        do it only once for every object id, that cache the resulting label for the rest of the video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1820/1820 [11:30<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "downscale_ratio = 0.4\n",
    "\n",
    "tracker = MultiTracker(dst_thresh=DST_THRESHOLD * downscale_ratio, tracker_func=cv2.legacy.TrackerKCF_create,\n",
    "                        upd_area_thresh=1.15)\n",
    "tracking_ad_results = {}\n",
    "\n",
    "ad_predictions = {}\n",
    "\n",
    "for frame_id, (frame_name, frame_path) in enumerate(tqdm(frames)):\n",
    "    frame = cv2.imread(frame_path)\n",
    "    downscale_size = (int(frame.shape[1] * downscale_ratio), int(frame.shape[0] * downscale_ratio))\n",
    "    downscaled_frame = cv2.resize(frame, downscale_size)\n",
    "    \n",
    "    tracker.update(downscaled_frame)\n",
    "    \n",
    "    if frame_id % DETECTION_RATE == 0:\n",
    "        # do object detection to add new objects\n",
    "        boxes = find_objects_bboxes(frame, AREA_LOWER_BOUND)\n",
    "        downscaled_boxes = np.array(boxes) * downscale_ratio\n",
    "        \n",
    "        tracker.add_boxes(downscaled_boxes, downscaled_frame)\n",
    "    \n",
    "    boxes = [(idx, (np.array(box) / downscale_ratio)) for idx, box in tracker.get_objects()]\n",
    "    \n",
    "    # filter overlapping boxes\n",
    "    overlapped_boxes = set()\n",
    "    for idx1, box1 in boxes:\n",
    "        for idx2, box2 in boxes:\n",
    "            if idx1 != idx2:\n",
    "                intersection = intersect_rectangles(box1, box2)\n",
    "                if intersection is not None and \\\n",
    "                        get_area(intersection) >= min(get_area(box1), get_area(box2)) * AREA_INTERSECTION_THRESH:\n",
    "                    if get_area(box1) < get_area(box2):\n",
    "                        overlapped_boxes.add(idx1)\n",
    "                    else:\n",
    "                        overlapped_boxes.add(idx2)\n",
    "    boxes = [(idx, box) for idx, box in boxes if idx not in overlapped_boxes]\n",
    "    \n",
    "    # add anomaly detection results\n",
    "    result = []\n",
    "    for idx, bbox in boxes:\n",
    "        if idx in ad_predictions:\n",
    "            is_anomaly, confidence = ad_predictions[idx]\n",
    "        else:\n",
    "            if not_touches_edge(bbox, frame.shape):\n",
    "                bbox = tuple(map(int, bbox))\n",
    "                image_bbox = crop_n_resize_image(frame, bbox, (IMAGE_WIDTH, IMAGE_HEIGHT), PAD_COLOR)\n",
    "                is_anomaly, confidence = get_prediction(client, image_bbox, project, model_version)\n",
    "                ad_predictions[idx] = is_anomaly, confidence\n",
    "            else:\n",
    "                # object is partially visible, do not use anomaly classification for this frame\n",
    "                # (will be done later, when objects fully enter the frame)\n",
    "                is_anomaly, confidence = False, None\n",
    "        result.append((idx, bbox, is_anomaly, confidence))\n",
    "        \n",
    "    tracking_ad_results[frame_name] = result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop AWS Lookout for vision model (to save costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: STOPPING_HOSTING\n"
     ]
    }
   ],
   "source": [
    "response = client.stop_model(ProjectName=project, ModelVersion=model_version)\n",
    "print('Status: ' + response['Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resulting frames and make a demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1820/1820 [03:39<00:00,  8.29it/s]\n"
     ]
    }
   ],
   "source": [
    "result_frames = make_frames_with_tracking_boxes(frames, tracking_ad_results, output_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1820/1820 [02:11<00:00, 13.80it/s]\n"
     ]
    }
   ],
   "source": [
    "save_video_from_frames(result_frames, result_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_anomaly",
   "language": "python",
   "name": "cv_anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
